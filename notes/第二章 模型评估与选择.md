---
attachments: [Clipboard_2021-10-07-14-29-17.png, Clipboard_2021-10-07-16-42-00.png, Clipboard_2021-10-07-16-42-10.png, Clipboard_2021-10-07-17-12-58.png, Clipboard_2021-10-07-17-13-11.png, Clipboard_2021-10-07-17-13-21.png, Clipboard_2021-10-07-17-13-30.png, Clipboard_2021-10-08-00-24-51.png, Clipboard_2021-10-08-00-25-54.png, Clipboard_2021-10-08-00-26-01.png, Clipboard_2021-10-08-00-30-08.png, Clipboard_2021-10-08-00-33-02.png, Clipboard_2021-10-08-00-38-01.png]
title: 第二章 模型评估与选择
created: '2021-10-03T17:22:41.869Z'
modified: '2021-10-07T16:39:53.660Z'
---

# 第二章 模型评估与选择
## 1. 经验误差与过拟合
* **错误率**：通常我们把分类错误的样本数占总样本总数的比例称为“错误率”
* **误差**：学习器的实际预测输出与样本的真实输出之间的差异称为“误差”
* **训练误差**：学习器在训练集上的误差
* **泛化误差**：在新样本上的误差
### 过拟合：
> 学习器把训练样本学的“太好”，可能把训练样本自身的一些特点当作了所有潜在样本都会具有的一般性质，导致泛化性能下降
* **原因**：
> 最常见的情况是学习能力过于强大，以至于把训练样本所包含的不太一般的特性都学到了
* 过拟合是机器学习面临的关键障碍，无法避免，只能缓解
### 欠拟合：
> 指对训练样本的一般性质尚未学好
* **原因**：
> 通常是由学习能力低下导致的。
* **欠拟合比较容易克服**：
> 1. 在决策树学习中扩展分支
> 2. 在神经网络学习中增加训练轮数等
## 2. 评估方法
### 测试集
* 首先，为了对学习器的泛化误差进行评估，我们需要一个“测试集”，以测试集上的“测试误差”作为泛化误差的近似
* 测试集应该尽可能与训练集互斥
* 几种常见的划分训练集S和测试集T的方法：

1. **留出法**

  > a. 直接将数据集D划分为两个互斥的集合
  > b. 训练/测试集的划分要尽可能**保持数据分布的一致性**，避免因数据划分过程引入额外的偏差而对最终结果产生影响。————通常采用“分层采样”
  > c. 单次使用留出法得到的估计结果往往不够稳定可靠，一般要采用若干次随即划分、重复进行试验评估后取平均值作为留出法的评估结果
  > d. 常见做法是将大约2/3~4/5的样本用于训练，剩余样本用于测试

2. **交叉验证法**

  > a. 先将数据集D划分为k个大小相似的互斥子集，每个子集Di都尽可能保持数据分布的一致性，即从D中通过分层采样得到。然后，每次用k-1个子集的并集作为训练集，余下的那个子集作为测试集。
  > b. 这样可以获得k组训练/测试集，从而可进行k次训练和测试，最终返回的是这k个测试结果的均值。
  > c. 交叉验证法评估结果的稳定性和保真性很大程度上取决于k的取值

3. **自助法**
  > a. 以自助采样法为基础：给定包含m个样本的数据集D，采样产生数据集D'。每次随即从D中挑选一个样本，将其拷贝放入D'，然后再将该样本放回初始数据集D中，使得该样本在下次采样时仍有可能被采到。这个过程执行m次后，我们就得到了包含m个样本的数据集D'。
  > b. 通过自助采样，初始数据集D中约有36.8%的样本未出现在采样数据集D'中。于是我们可将D'用作训练集，D\D'用作测试集。
  > 样本在m次采样中始终不被采到的概率为：
  ![](@attachment/Clipboard_2021-10-07-14-29-17.png)

4. **调参与最终模型**
  > a. 调参和算法选择没什么本质区别：对每种参数配置都训练出模型，然后把对应最好模型的参数作为结果。
  > b. 但事实上，对每种参数配置都训练出模型来是不可行的。现实中常用的做法是对每个参数选定一个范围和变化步长。
  * 最终模型
  > 在模型评估与选择过程中由于需要留出一部分数据进行评估测试，事实上我们只使用了一部分数据训练模型。因此，在模型选择完成后，学习算法和参数配置已选定，此时应该用数据集D重新训练模型。这个模型在训练过程中使用了所有m个样本，这才是我们最终提交给用户的模型。

## 3. 性能度量
* 对学习器的泛化性能进行评估，需要有衡量模型泛化能力的评价标准，这就是**性能度量**
* 最常用的为“**均方误差**”：
![](@attachment/Clipboard_2021-10-07-16-42-00.png)
* 更一般的情况，对于数据分布D和概率密度函数p(·)，可以描述为：
![](@attachment/Clipboard_2021-10-07-16-42-10.png)
### (1). 错误率与精度
#### **错误率**：
* 分类**错误**的样本数占样本总数的比例
![](@attachment/Clipboard_2021-10-07-17-12-58.png)
* 更一般的，对于数据分布D和概率密度函数p(·)，描述为：
![](@attachment/Clipboard_2021-10-07-17-13-11.png)
#### **精度**：
* 分类**正确**的样本数占样本总数的比例
![](@attachment/Clipboard_2021-10-07-17-13-21.png)
* 更一般的，对于数据分布D和概率密度函数p(·)，描述为：
![](@attachment/Clipboard_2021-10-07-17-13-30.png)
### (2). 查准率、查全率与F1
![](@attachment/Clipboard_2021-10-08-00-24-51.png)
* **查准率**：检索出的信息中有多少比例是用户感兴趣的
![](@attachment/Clipboard_2021-10-08-00-25-54.png)
* **查全率**：用户感兴趣的信息中有多少被检索出来了
![](@attachment/Clipboard_2021-10-08-00-26-01.png)
* 查准率和查全率是一对矛盾的度量。一般来说，查准率高时，查全率往往偏低；而查全率高时，查准率往往偏低。
#### **P-R**图：查准率-查全率曲线
![](@attachment/Clipboard_2021-10-08-00-30-08.png)
* 若一个学习器的P-R曲线被另一个学习器的曲线完全“包住”，则可断言后者的性能优于前者。
* **F1度量**：一个综合考虑查准率、查全率的性能度量
![](@attachment/Clipboard_2021-10-08-00-33-02.png)
* F1度量的一般形式 ———— F<sub>β<sub>
* 能让我们表达出对查准率/查全率的不同偏好，它定义为：
![](@attachment/Clipboard_2021-10-08-00-38-01.png)
* 其中β>0度量了查全率对查准率的相对重要性，β=1时退化为标准的F1；β>1时查全率有更大影响；β<1时查准率有更大影响
## 4. 比较检验
## 5. 偏差与方差
